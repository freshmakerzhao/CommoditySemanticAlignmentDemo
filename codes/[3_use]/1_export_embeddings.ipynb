{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d87a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPModel, CLIPProcessor, AutoModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9634d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 配置 =====================\n",
    "INPUT_CSV = \"../../train_master.csv\"\n",
    "CHECKPOINT_PATH = \"../[2_training]/checkpoints/best_model.pt\"  # 最佳模型\n",
    "OUTPUT_DIR = \"./embeddings\"\n",
    "BATCH_SIZE = 64\n",
    "MAX_TEXT_LEN = 128\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "IMAGE_MODEL = \"openai/clip-vit-large-patch14\"\n",
    "TEXT_MODEL = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "PROJ_DIM = 512\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 复用训练时的模型定义 =====================\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, image_model_name, text_model_name, proj_dim=512):\n",
    "        super().__init__()\n",
    "        clip_model = CLIPModel.from_pretrained(image_model_name)\n",
    "        self.image_encoder = clip_model.vision_model\n",
    "        img_dim = self.image_encoder.config.hidden_size\n",
    "        \n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        txt_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(img_dim, proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(txt_dim, proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def encode_image(self, pixel_values):\n",
    "        img_out = self.image_encoder(pixel_values=pixel_values)\n",
    "        img_feat = img_out.pooler_output\n",
    "        img_emb = F.normalize(self.image_proj(img_feat), dim=-1)\n",
    "        return img_emb\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        txt_feat = txt_out.pooler_output\n",
    "        txt_emb = F.normalize(self.text_proj(txt_feat), dim=-1)\n",
    "        return txt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574d28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 全量数据集（不区分 split）=====================\n",
    "class FullDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_processor, tokenizer, max_len=128):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 图像\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image_input = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # 文本\n",
    "        text = str(row[\"text\"]) if pd.notna(row[\"text\"]) else \"\"\n",
    "        text_input = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_len, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"id\": str(row[\"id\"]),\n",
    "            \"image\": image_input,\n",
    "            \"input_ids\": text_input[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_input[\"attention_mask\"].squeeze(0),\n",
    "            \"image_path\": row[\"image_path\"],\n",
    "            \"audio_path\": row[\"audio_path\"],\n",
    "            \"text\": text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1000\n",
      "Exporting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 16/16 [00:09<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "\n",
      "✓ Done! Exported 1000 entries to ./embeddings/\n",
      "  - text_embeddings.npy: (1000, 512)\n",
      "  - image_embeddings.npy: (1000, 512)\n",
      "  - metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Device: {DEVICE}\")\n",
    "print(\"Loading model...\")\n",
    "\n",
    "# 加载模型\n",
    "model = DualEncoder(IMAGE_MODEL, TEXT_MODEL, PROJ_DIM).to(DEVICE)\n",
    "state_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# 加载 processor 和 tokenizer\n",
    "image_processor = CLIPProcessor.from_pretrained(IMAGE_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)\n",
    "\n",
    "# 数据集\n",
    "dataset = FullDataset(INPUT_CSV, image_processor, tokenizer, MAX_TEXT_LEN)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "\n",
    "# 导出向量\n",
    "results = {\n",
    "    \"id\": [],\n",
    "    \"image_path\": [],\n",
    "    \"audio_path\": [],\n",
    "    \"text\": [],\n",
    "    \"text_emb\": [],\n",
    "    \"image_emb\": []\n",
    "}\n",
    "\n",
    "print(\"Exporting embeddings...\")\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Processing\"):\n",
    "        images = batch[\"image\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        text_emb = model.encode_text(input_ids, attention_mask).cpu().numpy()\n",
    "        image_emb = model.encode_image(images).cpu().numpy()\n",
    "        \n",
    "        results[\"id\"].extend(batch[\"id\"])\n",
    "        results[\"image_path\"].extend(batch[\"image_path\"])\n",
    "        results[\"audio_path\"].extend(batch[\"audio_path\"])\n",
    "        results[\"text\"].extend(batch[\"text\"])\n",
    "        results[\"text_emb\"].append(text_emb)\n",
    "        results[\"image_emb\"].append(image_emb)\n",
    "\n",
    "# 合并向量\n",
    "results[\"text_emb\"] = np.vstack(results[\"text_emb\"])\n",
    "results[\"image_emb\"] = np.vstack(results[\"image_emb\"])\n",
    "\n",
    "# 保存\n",
    "print(\"Saving...\")\n",
    "np.save(os.path.join(OUTPUT_DIR, \"text_embeddings.npy\"), results[\"text_emb\"])\n",
    "np.save(os.path.join(OUTPUT_DIR, \"image_embeddings.npy\"), results[\"image_emb\"])\n",
    "\n",
    "metadata = pd.DataFrame({\n",
    "    \"id\": results[\"id\"],\n",
    "    \"image_path\":  results[\"image_path\"],\n",
    "    \"audio_path\": results[\"audio_path\"],\n",
    "    \"text\":  results[\"text\"]\n",
    "})\n",
    "metadata.to_csv(os.path.join(OUTPUT_DIR, \"metadata.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n✓ Done! Exported {len(results['id'])} entries to {OUTPUT_DIR}/\")\n",
    "print(f\"  - text_embeddings.npy: {results['text_emb'].shape}\")\n",
    "print(f\"  - image_embeddings.npy: {results['image_emb'].shape}\")\n",
    "print(f\"  - metadata.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
