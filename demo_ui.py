import os
import gradio as gr
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import faiss
from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPProcessor
from PIL import Image
import whisper

# ===================== é…ç½® =====================
CHECKPOINT_PATH = "./codes/[2_training]/checkpoints/best_model.pt"
TEXT_INDEX_PATH = "./codes/[3_use]/indexes/text_flat.index"
IMAGE_INDEX_PATH = "./codes/[3_use]/indexes/image_flat.index"
METADATA_PATH = "./codes/[3_use]/embeddings/metadata.csv"

IMAGE_MODEL = "openai/clip-vit-large-patch14"
TEXT_MODEL = "hfl/chinese-roberta-wwm-ext-large"
PROJ_DIM = 512
MAX_TEXT_LEN = 128
DEVICE = "cuda"

# ===================== æ¨¡å‹å®šä¹‰ =====================
class DualEncoder(nn.Module):
    def __init__(self, image_model_name, text_model_name, proj_dim=512):
        super().__init__()
        clip_model = CLIPModel.from_pretrained(image_model_name)
        self.image_encoder = clip_model.vision_model
        img_dim = self.image_encoder.config.hidden_size
        
        self.text_encoder = AutoModel. from_pretrained(text_model_name)
        txt_dim = self.text_encoder.config.hidden_size
        
        self.image_proj = nn.Sequential(
            nn.Linear(img_dim, proj_dim),
            nn. GELU(),
            nn.Linear(proj_dim, proj_dim)
        )
        self.text_proj = nn.Sequential(
            nn.Linear(txt_dim, proj_dim),
            nn.GELU(),
            nn.Linear(proj_dim, proj_dim)
        )
        
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def encode_text(self, input_ids, attention_mask):
        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        txt_feat = txt_out.pooler_output
        txt_emb = F.normalize(self.text_proj(txt_feat), dim=-1)
        return txt_emb

    def encode_image(self, pixel_values):
        img_out = self.image_encoder(pixel_values=pixel_values)
        img_feat = img_out.pooler_output
        img_emb = F.normalize(self.image_proj(img_feat), dim=-1)
        return img_emb

# ===================== åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¨å±€ï¼ŒåªåŠ è½½ä¸€æ¬¡ï¼‰=====================
print("Loading models...")
model = DualEncoder(IMAGE_MODEL, TEXT_MODEL, PROJ_DIM).to(DEVICE)
state_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
model.load_state_dict(state_dict)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)
image_processor = CLIPProcessor.from_pretrained(IMAGE_MODEL)
whisper_model = whisper.load_model("base", device=DEVICE)

text_index = faiss.read_index(TEXT_INDEX_PATH)
image_index = faiss. read_index(IMAGE_INDEX_PATH)
metadata = pd.read_csv(METADATA_PATH)

print("âœ“ Models loaded!\n")

# ===================== æ£€ç´¢å‡½æ•° =====================
def search_by_text(query_text, search_mode, top_k):
    """æ–‡æœ¬æ£€ç´¢"""
    inputs = tokenizer(
        query_text,
        max_length=MAX_TEXT_LEN,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    ).to(DEVICE)
    
    with torch.no_grad():
        query_emb = model. encode_text(inputs["input_ids"], inputs["attention_mask"]).cpu().numpy()
    
    index = text_index if search_mode == "æ–‡æœ¬ç´¢å¼•" else image_index
    scores, indices = index.search(query_emb, top_k)
    
    return format_results(indices[0], scores[0])

def search_by_image(image, search_mode, top_k):
    """å›¾åƒæ£€ç´¢"""
    if image is None:
        return [], "âš ï¸ è¯·ä¸Šä¼ å›¾ç‰‡"
    
    image = Image.fromarray(image).convert("RGB")
    image_input = image_processor(images=image, return_tensors="pt")["pixel_values"].to(DEVICE)
    
    with torch.no_grad():
        query_emb = model. encode_image(image_input).cpu().numpy()
    
    index = image_index if search_mode == "å›¾åƒç´¢å¼•" else text_index
    scores, indices = index.search(query_emb, top_k)
    
    return format_results(indices[0], scores[0])

def search_by_audio(audio, search_mode, top_k):
    """è¯­éŸ³æ£€ç´¢"""
    if audio is None: 
        return [], "âš ï¸ è¯·ä¸Šä¼ éŸ³é¢‘"
    
    # Whisper è½¬å†™
    result = whisper_model.transcribe(audio, language="zh")
    query_text = result["text"].strip()
    
    if not query_text:
        return [], "âš ï¸ è¯­éŸ³è¯†åˆ«å¤±è´¥"
    
    # æ–‡æœ¬æ£€ç´¢
    inputs = tokenizer(
        query_text,
        max_length=MAX_TEXT_LEN,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    ).to(DEVICE)
    
    with torch.no_grad():
        query_emb = model.encode_text(inputs["input_ids"], inputs["attention_mask"]).cpu().numpy()
    
    index = text_index if search_mode == "æ–‡æœ¬ç´¢å¼•" else image_index
    scores, indices = index. search(query_emb, top_k)
    
    return format_results(indices[0], scores[0]), f"ğŸ¤ è¯†åˆ«ç»“æœ: {query_text}"

def format_results(indices, scores):
    """æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºå›¾ç‰‡åˆ—è¡¨"""
    results = []
    for idx, score in zip(indices, scores):
        row = metadata.iloc[idx]
        image_path = row["image_path"]
        
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if os.path.exists(image_path):
            results.append((
                image_path,
                f"ç›¸ä¼¼åº¦: {score:.3f}\nID: {row['id']}\n{row['text'][: 80]}..."
            ))
        else:
            # å›¾ç‰‡ä¸å­˜åœ¨æ—¶ç”¨å ä½ç¬¦
            results.append((
                None,
                f"âš ï¸ å›¾ç‰‡ç¼ºå¤±\nç›¸ä¼¼åº¦: {score:.3f}\nID: {row['id']}"
            ))
    
    return results

# ===================== Gradio ç•Œé¢ =====================
with gr.Blocks(title="å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # ğŸ” å¤šæ¨¡æ€å•†å“æ£€ç´¢ç³»ç»Ÿ
        æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ä¸‰ç§è¾“å…¥æ–¹å¼çš„è·¨æ¨¡æ€æ£€ç´¢
        """
    )
    
    with gr. Tabs():
        # ========== Tab 1: æ–‡æœ¬æ£€ç´¢ ==========
        with gr. Tab("æ–‡æœ¬æ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    text_input = gr.Textbox(
                        label="è¾“å…¥æŸ¥è¯¢æ–‡æœ¬",
                        placeholder="ä¾‹å¦‚:  é€æ˜æ‰‹æœºå£³",
                        lines=2
                    )
                    text_mode = gr.Radio(
                        choices=["æ–‡æœ¬ç´¢å¼•", "å›¾åƒç´¢å¼•"],
                        value="æ–‡æœ¬ç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    text_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    text_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                
                with gr.Column(scale=2):
                    text_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            text_btn.click(
                fn=search_by_text,
                inputs=[text_input, text_mode, text_topk],
                outputs=text_gallery
            )
        
        # ========== Tab 2: å›¾åƒæ£€ç´¢ ==========
        with gr.Tab("å›¾åƒæ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ æŸ¥è¯¢å›¾ç‰‡",
                        type="numpy",
                        height=300
                    )
                    image_mode = gr.Radio(
                        choices=["å›¾åƒç´¢å¼•", "æ–‡æœ¬ç´¢å¼•"],
                        value="å›¾åƒç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    image_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    image_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                
                with gr.Column(scale=2):
                    image_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            image_btn.click(
                fn=search_by_image,
                inputs=[image_input, image_mode, image_topk],
                outputs=image_gallery
            )
        
        # ========== Tab 3: è¯­éŸ³æ£€ç´¢ ==========
        with gr.Tab("ğŸ¤ è¯­éŸ³æ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    audio_input = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘æˆ–å½•éŸ³",
                        type="filepath"
                    )
                    audio_mode = gr.Radio(
                        choices=["æ–‡æœ¬ç´¢å¼•", "å›¾åƒç´¢å¼•"],
                        value="æ–‡æœ¬ç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    audio_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    audio_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                    audio_status = gr.Textbox(label="è¯†åˆ«çŠ¶æ€", interactive=False)
                
                with gr.Column(scale=2):
                    audio_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            audio_btn. click(
                fn=search_by_audio,
                inputs=[audio_input, audio_mode, audio_topk],
                outputs=[audio_gallery, audio_status]
            )
    
    gr.Markdown(
        """
        ---
        ### ä½¿ç”¨è¯´æ˜
        - **æ–‡æœ¬ç´¢å¼•**: åœ¨å•†å“æ–‡æœ¬æè¿°ä¸­æ£€ç´¢ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
        - **å›¾åƒç´¢å¼•**: åœ¨å•†å“å›¾ç‰‡ä¸­æ£€ç´¢ï¼ˆè§†è§‰ç›¸ä¼¼ï¼‰
        - **è·¨æ¨¡æ€**: æ–‡æœ¬æŸ¥å›¾ç‰‡ / å›¾ç‰‡æŸ¥æ–‡æœ¬ï¼ˆå¤šæ¨¡æ€å¯¹é½ï¼‰
        
        ### æ¨¡å‹ä¿¡æ¯
        - å›¾åƒç¼–ç å™¨: CLIP ViT-L/14
        - æ–‡æœ¬ç¼–ç å™¨: Chinese RoBERTa-Large
        - è¯­éŸ³è¯†åˆ«: Whisper Base
        - è®­ç»ƒæ•°æ®: 1000 æ¡å•†å“ (å›¾ç‰‡ + æ–‡æœ¬ + éŸ³é¢‘)
        """
    )

# ===================== å¯åŠ¨ =====================
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,
        share=False,  # æ”¹ä¸º True å¯ç”Ÿæˆå…¬ç½‘é“¾æ¥
        show_error=True
    )