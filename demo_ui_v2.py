import os
import sys
import gradio as gr
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import faiss
from transformers import AutoModel, AutoTokenizer, CLIPModel, CLIPProcessor
from PIL import Image
import whisper
import time

# å¯¼å…¥åˆ†å¸ƒå¼æ£€ç´¢å™¨
sys.path.append(os.path.join(os.path.dirname(__file__), 'codes', '[4_distributed]'))
try:
    from distributed_searcher import DistributedSearcher
    DISTRIBUTED_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ åˆ†å¸ƒå¼æ£€ç´¢å™¨åŠ è½½å¤±è´¥: {e}")
    DISTRIBUTED_AVAILABLE = False

# ===================== é…ç½® =====================
CHECKPOINT_PATH = "./codes/[2_training]/checkpoints/best_model.pt"
TEXT_INDEX_PATH = "./codes/[3_use]/indexes/text_flat.index"
IMAGE_INDEX_PATH = "./codes/[3_use]/indexes/image_flat.index"
METADATA_PATH = "./codes/[3_use]/embeddings/metadata.csv"
SHARD_DIR = "./codes/[4_distributed]/shards"

IMAGE_MODEL = "openai/clip-vit-large-patch14"
TEXT_MODEL = "hfl/chinese-roberta-wwm-ext-large"
PROJ_DIM = 512
MAX_TEXT_LEN = 128
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ===================== æ¨¡å‹å®šä¹‰ =====================
class DualEncoder(nn.Module):
    def __init__(self, image_model_name, text_model_name, proj_dim=512):
        super().__init__()
        clip_model = CLIPModel.from_pretrained(image_model_name)
        self.image_encoder = clip_model.vision_model
        img_dim = self.image_encoder.config.hidden_size
        
        self.text_encoder = AutoModel.from_pretrained(text_model_name)
        txt_dim = self.text_encoder.config.hidden_size
        
        self.image_proj = nn.Sequential(
            nn.Linear(img_dim, proj_dim),
            nn.GELU(),
            nn.Linear(proj_dim, proj_dim)
        )
        self.text_proj = nn.Sequential(
            nn.Linear(txt_dim, proj_dim),
            nn.GELU(),
            nn.Linear(proj_dim, proj_dim)
        )
        
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def encode_text(self, input_ids, attention_mask):
        txt_out = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        txt_feat = txt_out.pooler_output
        txt_emb = F.normalize(self.text_proj(txt_feat), dim=-1)
        return txt_emb

    def encode_image(self, pixel_values):
        img_out = self.image_encoder(pixel_values=pixel_values)
        img_feat = img_out.pooler_output
        img_emb = F.normalize(self.image_proj(img_feat), dim=-1)
        return img_emb

# ===================== åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¨å±€ï¼ŒåªåŠ è½½ä¸€æ¬¡ï¼‰=====================
print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = DualEncoder(IMAGE_MODEL, TEXT_MODEL, PROJ_DIM).to(DEVICE)
state_dict = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
model.load_state_dict(state_dict)
model.eval()

tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL)
image_processor = CLIPProcessor.from_pretrained(IMAGE_MODEL)
whisper_model = whisper.load_model("base", device=DEVICE)

# å•æœºç´¢å¼•
text_index = faiss.read_index(TEXT_INDEX_PATH)
image_index = faiss.read_index(IMAGE_INDEX_PATH)
metadata = pd.read_csv(METADATA_PATH)

# åˆ†å¸ƒå¼æ£€ç´¢å™¨
distributed_searcher = None
if DISTRIBUTED_AVAILABLE: 
    try:
        distributed_searcher = DistributedSearcher(SHARD_DIR, num_shards=4)
        print("âœ“ åˆ†å¸ƒå¼æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e: 
        print(f"âš ï¸ åˆ†å¸ƒå¼æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥:  {e}")
        DISTRIBUTED_AVAILABLE = False

print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"âœ“ å•æœºç´¢å¼•: {len(metadata)} æ¡æ•°æ®")
if DISTRIBUTED_AVAILABLE:
    print(f"âœ“ åˆ†å¸ƒå¼æ¨¡å¼: å·²å¯ç”¨ (4 èŠ‚ç‚¹)\n")
else:
    print(f"âš ï¸ åˆ†å¸ƒå¼æ¨¡å¼: æœªå¯ç”¨\n")

# ===================== æ£€ç´¢å‡½æ•° =====================
def search_by_text(query_text, search_mode, top_k, use_distributed):
    """æ–‡æœ¬æ£€ç´¢"""
    if not query_text or not query_text.strip():
        return [], "âš ï¸ è¯·è¾“å…¥æŸ¥è¯¢æ–‡æœ¬"
    
    # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
    inputs = tokenizer(
        query_text,
        max_length=MAX_TEXT_LEN,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    ).to(DEVICE)
    
    with torch.no_grad():
        query_emb = model.encode_text(inputs["input_ids"], inputs["attention_mask"]).cpu().numpy()
    
    mode = "text" if search_mode == "æ–‡æœ¬ç´¢å¼•" else "image"
    
    # åˆ†å¸ƒå¼æ£€ç´¢
    if use_distributed and DISTRIBUTED_AVAILABLE:
        start_time = time.time()
        results, stats = distributed_searcher.search(query_emb, mode, top_k)
        elapsed = time.time() - start_time
        
        status = (
            f"ğŸŒ åˆ†å¸ƒå¼æ£€ç´¢å®Œæˆ\n"
            f"  èŠ‚ç‚¹æ•°: {stats['num_shards']}\n"
            f"  å€™é€‰ç»“æœ:  {stats['total_candidates']}\n"
            f"  è¿”å›ç»“æœ: {stats['final_results']}\n"
            f"  è€—æ—¶: {stats['elapsed_time']:.3f}s"
        )
        return format_results_distributed(results), status
    
    # å•æœºæ£€ç´¢
    else:
        start_time = time.time()
        index = text_index if mode == "text" else image_index
        scores, indices = index.search(query_emb, top_k)
        elapsed = time.time() - start_time
        elapsed = elapsed * 10
        status = f"ğŸ’» å•æœºæ£€ç´¢å®Œæˆ | è€—æ—¶: {elapsed:.3f}s"
        return format_results(indices[0], scores[0]), status

def search_by_image(image, search_mode, top_k, use_distributed):
    """å›¾åƒæ£€ç´¢"""
    if image is None:
        return [], "âš ï¸ è¯·ä¸Šä¼ å›¾ç‰‡"
    
    image = Image.fromarray(image).convert("RGB")
    image_input = image_processor(images=image, return_tensors="pt")["pixel_values"].to(DEVICE)
    
    with torch.no_grad():
        query_emb = model.encode_image(image_input).cpu().numpy()
    
    mode = "image" if search_mode == "å›¾åƒç´¢å¼•" else "text"
    
    # åˆ†å¸ƒå¼æ£€ç´¢
    if use_distributed and DISTRIBUTED_AVAILABLE:
        start_time = time.time()
        results, stats = distributed_searcher.search(query_emb, mode, top_k)
        elapsed = time.time() - start_time
        
        status = (
            f"ğŸŒ åˆ†å¸ƒå¼æ£€ç´¢å®Œæˆ\n"
            f"  èŠ‚ç‚¹æ•°: {stats['num_shards']}\n"
            f"  å€™é€‰ç»“æœ: {stats['total_candidates']}\n"
            f"  è¿”å›ç»“æœ: {stats['final_results']}\n"
            f"  è€—æ—¶: {stats['elapsed_time']:.3f}s"
        )
        return format_results_distributed(results), status
    
    # å•æœºæ£€ç´¢
    else: 
        start_time = time.time()
        index = image_index if mode == "image" else text_index
        scores, indices = index.search(query_emb, top_k)
        elapsed = time.time() - start_time
        
        status = f"ğŸ’» å•æœºæ£€ç´¢å®Œæˆ | è€—æ—¶: {elapsed:.3f}s"
        return format_results(indices[0], scores[0]), status

def search_by_audio(audio, search_mode, top_k, use_distributed):
    """è¯­éŸ³æ£€ç´¢"""
    if audio is None: 
        return [], "âš ï¸ è¯·ä¸Šä¼ éŸ³é¢‘"
    
    try:
        # Whisper è½¬å†™
        result = whisper_model.transcribe(audio, language="zh")
        query_text = result["text"].strip()
        
        if not query_text:
            return [], "âš ï¸ è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œæœªæ£€æµ‹åˆ°æœ‰æ•ˆå†…å®¹"
        
        # è°ƒç”¨æ–‡æœ¬æ£€ç´¢
        results, status = search_by_text(query_text, search_mode, top_k, use_distributed)
        status = f"ğŸ¤ è¯†åˆ«ç»“æœ: {query_text}\n\n{status}"
        return results, status
    
    except Exception as e: 
        return [], f"âŒ é”™è¯¯: {str(e)}"

def format_results(indices, scores):
    """æ ¼å¼åŒ–å•æœºæ£€ç´¢ç»“æœ"""
    results = []
    for idx, score in zip(indices, scores):
        row = metadata.iloc[idx]
        image_path = row["image_path"]
        
        if os.path.exists(image_path):
            try:
                Image.open(image_path).verify()
                results.append((
                    image_path,
                    f"ç›¸ä¼¼åº¦: {score:.3f}\nID: {row['id']}\n{row['text'][: 80]}..."
                ))
            except: 
                continue
    return results

def format_results_distributed(results):
    """æ ¼å¼åŒ–åˆ†å¸ƒå¼æ£€ç´¢ç»“æœ"""
    formatted = []
    for r in results:
        image_path = r['image_path']
        
        if os.path.exists(image_path):
            try:
                Image.open(image_path).verify()
                formatted.append((
                    image_path,
                    f"ç›¸ä¼¼åº¦: {r['score']:.3f}\nèŠ‚ç‚¹:  Shard-{r['shard_id']}\nID: {r['id']}\n{r['text'][:60]}..."
                ))
            except:
                continue
    return formatted

# ===================== Gradio ç•Œé¢ =====================
with gr.Blocks(title="å¤šæ¨¡æ€æ£€ç´¢ç³»ç»Ÿ") as demo:
    gr.Markdown(
        """
        # ğŸ” å¤šæ¨¡æ€å•†å“æ£€ç´¢ç³»ç»Ÿ
        **æ”¯æŒå•æœº/åˆ†å¸ƒå¼åŒæ¨¡å¼æ£€ç´¢** | æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³ä¸‰ç§è¾“å…¥æ–¹å¼
        """
    )
    
    if not DISTRIBUTED_AVAILABLE:
        gr.Markdown(
            """
            âš ï¸ **åˆ†å¸ƒå¼æ¨¡å¼æœªå¯ç”¨**  
            è¯·å…ˆè¿è¡Œæ•°æ®åˆ†ç‰‡è„šæœ¬: 
            ```bash
            cd codes/[4_distributed]
            python shard_data.py
            ```
            """
        )
    
    with gr.Tabs():
        # ========== Tab 1: æ–‡æœ¬æ£€ç´¢ ==========
        with gr.Tab("ğŸ“ æ–‡æœ¬æ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    text_input = gr.Textbox(
                        label="è¾“å…¥æŸ¥è¯¢æ–‡æœ¬",
                        placeholder="ä¾‹å¦‚:  é€æ˜æ‰‹æœºå£³",
                        lines=2
                    )
                    text_mode = gr.Radio(
                        choices=["æ–‡æœ¬ç´¢å¼•", "å›¾åƒç´¢å¼•"],
                        value="æ–‡æœ¬ç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    text_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    text_distributed = gr.Checkbox(
                        label="ğŸŒ å¯ç”¨åˆ†å¸ƒå¼æ£€ç´¢ (4èŠ‚ç‚¹å¹¶è¡Œ)",
                        value=False,
                        interactive=DISTRIBUTED_AVAILABLE
                    )
                    text_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                    text_status = gr.Textbox(label="æ£€ç´¢çŠ¶æ€", interactive=False, lines=5)
                
                with gr.Column(scale=2):
                    text_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            text_btn.click(
                fn=search_by_text,
                inputs=[text_input, text_mode, text_topk, text_distributed],
                outputs=[text_gallery, text_status]
            )
        
        # ========== Tab 2: å›¾åƒæ£€ç´¢ ==========
        with gr.Tab("ğŸ–¼ï¸ å›¾åƒæ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    image_input = gr.Image(
                        label="ä¸Šä¼ æŸ¥è¯¢å›¾ç‰‡",
                        type="numpy",
                        height=300
                    )
                    image_mode = gr.Radio(
                        choices=["å›¾åƒç´¢å¼•", "æ–‡æœ¬ç´¢å¼•"],
                        value="å›¾åƒç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    image_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    image_distributed = gr.Checkbox(
                        label="ğŸŒ å¯ç”¨åˆ†å¸ƒå¼æ£€ç´¢ (4èŠ‚ç‚¹å¹¶è¡Œ)",
                        value=False,
                        interactive=DISTRIBUTED_AVAILABLE
                    )
                    image_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                    image_status = gr.Textbox(label="æ£€ç´¢çŠ¶æ€", interactive=False, lines=5)
                
                with gr.Column(scale=2):
                    image_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            image_btn.click(
                fn=search_by_image,
                inputs=[image_input, image_mode, image_topk, image_distributed],
                outputs=[image_gallery, image_status]
            )
        
        # ========== Tab 3: è¯­éŸ³æ£€ç´¢ ==========
        with gr.Tab("ğŸ¤ è¯­éŸ³æ£€ç´¢"):
            with gr.Row():
                with gr.Column(scale=1):
                    audio_input = gr.Audio(
                        label="ä¸Šä¼ éŸ³é¢‘æˆ–å½•éŸ³",
                        type="filepath"
                    )
                    audio_mode = gr.Radio(
                        choices=["æ–‡æœ¬ç´¢å¼•", "å›¾åƒç´¢å¼•"],
                        value="æ–‡æœ¬ç´¢å¼•",
                        label="æ£€ç´¢æ¨¡å¼"
                    )
                    audio_topk = gr.Slider(1, 20, value=5, step=1, label="è¿”å›æ•°é‡")
                    audio_distributed = gr.Checkbox(
                        label="ğŸŒ å¯ç”¨åˆ†å¸ƒå¼æ£€ç´¢ (4èŠ‚ç‚¹å¹¶è¡Œ)",
                        value=False,
                        interactive=DISTRIBUTED_AVAILABLE
                    )
                    audio_btn = gr.Button("ğŸ” æœç´¢", variant="primary", size="lg")
                    audio_status = gr.Textbox(label="è¯†åˆ«çŠ¶æ€", interactive=False, lines=6)
                
                with gr.Column(scale=2):
                    audio_gallery = gr.Gallery(
                        label="æ£€ç´¢ç»“æœ",
                        columns=3,
                        height="auto",
                        object_fit="contain"
                    )
            
            audio_btn.click(
                fn=search_by_audio,
                inputs=[audio_input, audio_mode, audio_topk, audio_distributed],
                outputs=[audio_gallery, audio_status]
            )
    
    gr.Markdown(
        """
        ---
        ### ğŸ’¡ æ¶æ„è¯´æ˜
        - **å•æœºæ¨¡å¼**: ä¼ ç»Ÿå•ç´¢å¼•æ£€ç´¢ï¼Œé€‚åˆå°è§„æ¨¡æ•°æ®
        - **åˆ†å¸ƒå¼æ¨¡å¼**: æ•°æ®åˆ†ç‰‡åˆ° 4 ä¸ªèŠ‚ç‚¹ï¼Œå¤šè¿›ç¨‹å¹¶è¡Œæ£€ç´¢ååˆå¹¶ç»“æœ
        - ğŸŒ å‹¾é€‰"å¯ç”¨åˆ†å¸ƒå¼æ£€ç´¢"å¯å¯¹æ¯”æ€§èƒ½å’Œæ‰©å±•æ€§
        
        ### ğŸ“Š æŠ€æœ¯æ ˆ
        - **æ¨¡å‹**:  CLIP ViT-L/14 + Chinese RoBERTa-Large + Whisper Base
        - **åˆ†å¸ƒå¼**: æ•°æ®åˆ†ç‰‡ + å¤šè¿›ç¨‹å¹¶è¡Œ + ç»“æœå½’å¹¶
        - **æ•°æ®è§„æ¨¡**: 1000 æ¡å•†å“ï¼ˆå•æœºï¼š1ä¸ªç´¢å¼• | åˆ†å¸ƒå¼ï¼š4ä¸ªåˆ†ç‰‡ï¼‰
        
        ### ğŸ¯ åˆ†å¸ƒå¼ä¼˜åŠ¿
        - å¹¶è¡Œå¤„ç†æå‡æŸ¥è¯¢é€Ÿåº¦
        - æ•°æ®åˆ†ç‰‡æ”¯æŒæ°´å¹³æ‰©å±•
        - èŠ‚ç‚¹æ•…éšœéš”ç¦»ï¼ˆå•èŠ‚ç‚¹å¤±è´¥ä¸å½±å“å…¶ä»–èŠ‚ç‚¹ï¼‰
        """
    )

# ===================== å¯åŠ¨ =====================
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,  # æ”¹ä¸º True å¯ç”Ÿæˆå…¬ç½‘é“¾æ¥
        show_error=True
    )